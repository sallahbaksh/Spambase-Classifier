{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Spam Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from prepare_data import *\n",
    "from statistics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sigmoid(x, theta):\n",
    "    return 1 / (1 + np.exp(-x @ theta))\n",
    "\n",
    "def calc_cost(sigmoid, train_y, N):\n",
    "    return -1/N * (train_y.T @ np.log(sigmoid) + np.subtract(1, train_y).T @ np.subtract(1, np.log(sigmoid)))\n",
    "\n",
    "def logistic_regression(s_train_x, train_y):\n",
    "    \n",
    "    # initialize the parameters of theta using random values in the range [-1, 1]\n",
    "    thetas = np.random.uniform(-1, 1, (s_train_x.shape[1], 1))\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    change_termination = math.pow(2, -23)\n",
    "    current_iteration, max_iterations = 0, 1500\n",
    "    N = s_train_x.shape[0]\n",
    "\n",
    "    previous_cost = 0\n",
    "\n",
    "    while current_iteration < max_iterations:\n",
    "\n",
    "        # update each parameter using batch gradient descent\n",
    "        gradient = s_train_x.T @ np.subtract(train_y, calc_sigmoid(s_train_x, thetas))\n",
    "        thetas += learning_rate/N * gradient\n",
    "\n",
    "        current_cost = calc_cost(calc_sigmoid(s_train_x, thetas), train_y, N)\n",
    "\n",
    "        # if absolute value change in the loss on the data is less than 2^(âˆ’23) terminate loop    \n",
    "        if np.abs(current_cost - previous_cost) < change_termination:\n",
    "            break\n",
    "\n",
    "        previous_cost = current_cost\n",
    "        current_iteration += 1   \n",
    "    \n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  88.25831702544032 %\n",
      "precision:  84.13793103448276 %\n",
      "recall:  84.72222222222221 %\n",
      "f_measure:  84.42906574394463 %\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y = split_data()\n",
    "s_train_x, s_test_x = standardize_data(train_x, test_x, True)\n",
    "\n",
    "thetas = logistic_regression(s_train_x, train_y)\n",
    "sigmoid = calc_sigmoid(s_test_x, thetas)\n",
    "\n",
    "for prediction in range(0, len(sigmoid)):\n",
    "    if sigmoid[prediction] < 0.5:\n",
    "        sigmoid[prediction] = 0\n",
    "    else:\n",
    "        sigmoid[prediction] = 1\n",
    "        \n",
    "TP, TN, FP, FN = confusion_matrix(sigmoid, test_y)      \n",
    "print(\"accuracy: \", calc_accuracy(TP, TN, test_y.shape[0]) * 100, \"%\")\n",
    "print(\"precision: \", calc_precision(TP, FP) * 100, \"%\")\n",
    "print(\"recall: \", calc_recall(TP, FN) * 100, \"%\")\n",
    "print(\"f_measure: \", calc_f_measure(TP, FP, FN) * 100, \"%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
